{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Transform(inputString, lower = False):\n",
    "    \"\"\"\n",
    "    Removes puctuation marks, lowers and splits the string into words and removes stop words from the list\n",
    "    \"\"\"\n",
    "    # remove punctuation marks from string\n",
    "    string = re.sub(\"[^a-zA-Z0-9]\", \" \", inputString)\n",
    "    \n",
    "    if(lower):\n",
    "        string = string.lower()\n",
    "    \n",
    "    #split the string\n",
    "    words = string.split()\n",
    "    \n",
    "    #remove stop words\n",
    "    stopwrds = set(stopwords.words(\"english\"))\n",
    "    noSW = [w for w in words if w not in stopwrds]\n",
    "    \n",
    "    return noSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findSynonyms(word1, level = 'start'):\n",
    "    \"\"\"\n",
    "    returns a list of the synynoms of the word by considering the level(start or end)\n",
    "    \"\"\"\n",
    "    tofind = \"http://api.conceptnet.io/c/en/\" + word1\n",
    "    obj = requests.get(tofind).json()\n",
    "    \n",
    "    #copy all the snynonyms in a list\n",
    "    synWord = []\n",
    "    \n",
    "    for i in range(len(obj['edges'])):\n",
    "        synWord.append(obj['edges'][i][level]['label'])\n",
    "\n",
    "    return synWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming and lemitization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemming(synWord):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Stemming\n",
    "    StemmedSynWord = []\n",
    "    for idx, w in enumerate(synWord):\n",
    "        w = word_tokenize(w)\n",
    "        tempword = \"\"\n",
    "        for index, every in enumerate(w):\n",
    "            tempword = tempword + (ps.stem(every))\n",
    "            if (index != len(w) - 1):\n",
    "                tempword = tempword + \" \"\n",
    "        if(tempword not in StemmedSynWord):\n",
    "            StemmedSynWord.append(tempword)\n",
    "\n",
    "    return StemmedSynWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmitization(StemmedSynWord):\n",
    "    wl = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatization\n",
    "    lemmaSynWord = []\n",
    "    for idx, w in enumerate(StemmedSynWord):\n",
    "        w = word_tokenize(w)\n",
    "        tempword = \"\"\n",
    "        for index, every in enumerate(w):\n",
    "            tempword = tempword + (wl.lemmatize(every))\n",
    "            if (index != len(w) - 1):\n",
    "                tempword = tempword + \" \"\n",
    "        if(tempword not in lemmaSynWord):\n",
    "            lemmaSynWord.append(tempword)\n",
    "\n",
    "    return lemmaSynWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding synonyms of the word search...\n",
      "Stemming...\n",
      "Lemmitizating...\n",
      "Resultant list: \n",
      "['find a lost item', 'find', 'find inform', 'rout up', 'explor', 'forag', 'frisk', 'hunt', 'look', 'manhunt', 'pursuit', 'quest', 'ransack', 'scour', 'search']\n",
      "\n",
      "\n",
      "Finding synonyms of the word add...\n",
      "Stemming...\n",
      "Lemmitizating...\n",
      "Resultant list: \n",
      "['add', 'comput a sum', 'add on', 'adjoin', 'button', 'butyl', 'compound', 'concaten', 'enrich', 'foot', 'fortifi', 'gild the lili', 'includ', 'inject', 'intercal']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# words whose synonyms are to be found\n",
    "word1 = \"search\"\n",
    "word2 = \"add\"\n",
    "\n",
    "wordList = [word1, word2]\n",
    "\n",
    "synWord = []\n",
    "for word in wordList:\n",
    "    print(\"Finding synonyms of the word \" + word + \"...\")\n",
    "    synword = findSynonyms(word)\n",
    "\n",
    "    print(\"Stemming...\")\n",
    "    StemmedSynWord = stemming(synword)\n",
    "\n",
    "    print(\"Lemmitizating...\")\n",
    "    lemmaSynWord = lemmitization(StemmedSynWord)\n",
    "\n",
    "    print(\"Resultant list: \")\n",
    "    print(lemmaSynWord)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    synWord.append(lemmaSynWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string1 = \"look for bindu singh\"\n",
    "string2 = \"Can you search for himanshu singh\"\n",
    "string3 = \"LinkedIn, add an experience in my profile\"\n",
    "string4 = \"find Himans\"\n",
    "string5 = \"What even himan?\"\n",
    "# string6 = \"I want to see Himan's profile\"\n",
    "string7 = \"Look for job openings at LinkedIn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranformed string is: ['Can', 'search', 'himanshu', 'singh']\n",
      "Operation: search\n"
     ]
    }
   ],
   "source": [
    "# for number in range(1, 6):\n",
    "words = Transform(string2)\n",
    "\n",
    "print(\"Tranformed string is: \" + str(words))\n",
    "\n",
    "flag = 0\n",
    "for word in words:\n",
    "    for idx, synonyms in enumerate(synWord): \n",
    "        if(word in synonyms):        \n",
    "            print(\"Operation: \" + wordList[idx])\n",
    "            flag = 1\n",
    "            break\n",
    "        \n",
    "    if(flag == 1):\n",
    "        break\n",
    "\n",
    "if(flag == 0):\n",
    "    print(\"You are not speaking a valid operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Look for himanshu singh'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string  = string1.capitalize()\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are There Any Senior Software Developer Positions At Linkedin'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying to capitalise the first letter of each letter \n",
    "# stringr = \"Look for job openings at linkedIn\"\n",
    "# stringr2 = \"are there any senior software developer positions at linkedin\"\n",
    "# string = word_tokenize(stringr2)\n",
    "# stri = \"\"\n",
    "\n",
    "# for idx, s in enumerate(string):\n",
    "#     s = s.capitalize()\n",
    "#     stri = stri + s\n",
    "#     if(idx != len(string) - 1):\n",
    "#         stri += \" \"\n",
    "\n",
    "# stri    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 'VBP'),\n",
       " ('there', 'RB'),\n",
       " ('any', 'DT'),\n",
       " ('senior', 'JJ'),\n",
       " ('software', 'NN'),\n",
       " ('developer', 'NN'),\n",
       " ('positions', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('linkedin', 'NN')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stri = \"search for bindu singh\"\n",
    "striToken = word_tokenize(stringr2)\n",
    "nltk.pos_tag(striToken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
